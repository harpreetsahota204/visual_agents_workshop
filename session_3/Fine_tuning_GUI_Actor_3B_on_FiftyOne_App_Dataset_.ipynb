{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSPH_2aycMCO"
      },
      "source": [
        "# Fine-tuning GUI-Actor 3B on FiftyOne App Dataset\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/visual_agents_workshop/blob/main/session_3/Fine_tuning_GUI_Actor_3B_on_FiftyOne_App_Dataset_.ipynb)\n",
        "\n",
        "\n",
        "## 📋 Prerequisites:\n",
        "- **GPU Runtime**: Select GPU in `Runtime` → `Change runtime type`\n",
        "- **Hugging Face Account**: For accessing models and datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOu_2kKKmMqC"
      },
      "source": [
        "# 📦 Installation & Setup\n",
        "\n",
        "First, let's install all the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYTY7i4vHH3K"
      },
      "outputs": [],
      "source": [
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ2vj02kT2uk"
      },
      "outputs": [],
      "source": [
        "!pip install -e git+https://github.com/harpreetsahota204/GUI-Actor-for-FiftyOne.git#egg=gui_actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "046FXiTJNZjh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import os\n",
        "import fiftyone as fo\n",
        "import fiftyone.utils.huggingface as fouh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY9JNWcxOGIX"
      },
      "source": [
        "## Load an SFT Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7FiO9ApBeqJ",
        "outputId": "27bcaee3-f80a-440d-ab54-bf3f5ed61de4"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"harpreetsahota/FiftyOne-GUI-Grounding-Train-with-Synthetic\",\n",
        "    overwrite=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKOB2OrS0yi",
        "outputId": "cc5744c9-00b9-484e-f87e-775f23cec618"
      },
      "outputs": [],
      "source": [
        "dataset.first().keypoints.keypoints[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrcgMwnsgg5-"
      },
      "source": [
        "### Bridging Two Worlds: FiftyOne Annotations → GUI-Actor Conversations\n",
        "\n",
        "A fundamental challenge is that FiftyOne organizes data around visual annotations - keypoints mark where to click, bounding boxes show regions to select.\n",
        "\n",
        "But GUI-Actor expects conversational training data where each interaction is a dialogue between user and assistant.\n",
        "\n",
        "This means we must creat an entire transformation pipeline that could take a single screenshot with multiple annotations and convert it into multiple training conversations, each with proper system prompts, user instructions, and assistant responses that include coordinate ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgsFovHoMdSr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "KP_SYSTEM_MESSAGE = \"\"\"You are a GUI Agent specialized in interacting with the FiftyOne application. Given a screenshot of the current FiftyOne GUI and a human instruction, your task is to locate the screen element that corresponds to the instruction.\n",
        "\n",
        "You should output a response that:\n",
        "1. Describes the action you will take in natural language\n",
        "2. Provides the exact coordinates where you will interact\n",
        "3. Includes a valid JSON object with the action details, element information, interaction points, and any relevant metadata\n",
        "\n",
        "The JSON should contain fields for action, element_info, points (as coordinate arrays), and custom_metadata.\"\"\"\n",
        "\n",
        "BB_SYSTEM_MESSAGE = \"\"\"You are a GUI Agent specialized in interacting with the FiftyOne application. Given a screenshot of the current FiftyOne GUI and a human instruction, your task is to locate the screen element that corresponds to the instruction.\n",
        "\n",
        "You should output a response that:\n",
        "1. Describes the action you will take in natural language\n",
        "2. Provides the bounding box coordinates for the interaction region\n",
        "3. Includes a valid JSON object with the action details, element information, bounding box coordinates, and any relevant metadata\n",
        "\n",
        "The JSON should contain fields for action, element_info, bounding_box (as [x_min, y_min, x_max, y_max]), and custom_metadata.\"\"\"\n",
        "\n",
        "def add_message_payload_to_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Add message payloads to FiftyOne dataset annotations for GUI-Actor training.\n",
        "    \n",
        "    This function processes both keypoint (single-point) and detection (bounding box)\n",
        "    annotations, converting them into conversation format for vision-language model training.\n",
        "    Each annotation gets a complete conversation with system prompt, user query, and \n",
        "    assistant response containing both natural language and structured JSON.\n",
        "    \n",
        "    Args:\n",
        "        dataset: FiftyOne dataset with 'keypoints' and/or 'detections' fields\n",
        "        \n",
        "    Modifies:\n",
        "        Adds 'message_payload' field to each annotation containing the formatted\n",
        "        conversation for training\n",
        "    \"\"\"\n",
        "    \n",
        "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
        "        filepath = sample[\"filepath\"]\n",
        "        \n",
        "        # Process keypoints (point-based interactions like clicks)\n",
        "        if sample.keypoints:\n",
        "            for kp in sample.keypoints.keypoints:\n",
        "                # Extract keypoint attributes with safe defaults\n",
        "                task_desc = getattr(kp, 'task_description', '')\n",
        "                element_info = getattr(kp, 'element_info', None)\n",
        "                action = getattr(kp, 'label', 'click')  # Default action is click\n",
        "                points = getattr(kp, 'points', [])\n",
        "                custom_metadata = getattr(kp, 'custom_metadata', None)\n",
        "                \n",
        "                # Ensure element_info is a proper dict or string\n",
        "                if element_info is None or element_info == {} or element_info == '':\n",
        "                    element_info = \"ui_element\"  # Simple string fallback\n",
        "                elif isinstance(element_info, dict) and not element_info:\n",
        "                    element_info = \"ui_element\"  # Empty dict -> string\n",
        "                elif not isinstance(element_info, (dict, str)):\n",
        "                    element_info = str(element_info)  # Convert to string if weird type\n",
        "                \n",
        "                # Ensure custom_metadata is a proper dict\n",
        "                if custom_metadata is None or custom_metadata == {} or custom_metadata == '':\n",
        "                    custom_metadata = {\"type\": \"point_interaction\"}\n",
        "                elif not isinstance(custom_metadata, dict):\n",
        "                    custom_metadata = {\"value\": str(custom_metadata)}\n",
        "                \n",
        "                # Only process if we have valid coordinates\n",
        "                if points and len(points) > 0:\n",
        "                    x, y = points[0]\n",
        "                    \n",
        "                    # Build the JSON response object\n",
        "                    json_response = {\n",
        "                        \"action\": action,\n",
        "                        \"element_info\": element_info,\n",
        "                        \"points\": [[round(x, 4), round(y, 4)]],  # Round for cleaner output\n",
        "                        \"custom_metadata\": custom_metadata\n",
        "                    }\n",
        "                    \n",
        "                    # Create natural language response with embedded JSON\n",
        "                    # Note: coordinates in text match those used for pointer tokens\n",
        "                    response_text = f\"\"\"I will {action} the {element_info if isinstance(element_info, str) else 'element'}. ```json {json.dumps(json_response)}```\"\"\"\n",
        "                    \n",
        "                    # Create the full conversation format\n",
        "                    messages = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": KP_SYSTEM_MESSAGE}\n",
        "                            ]\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"image\", \"image\": filepath},\n",
        "                                {\"type\": \"text\", \"text\": task_desc}\n",
        "                            ]\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": response_text}\n",
        "                            ],\n",
        "                            \"recipient\": \"os\",\n",
        "                            \"end_turn\": True,\n",
        "                            \"point_gt\": points[0] if points else None  # Ground truth for pointer loss\n",
        "                        }\n",
        "                    ]\n",
        "                    \n",
        "                    kp.message_payload = messages\n",
        "        \n",
        "        # Process detections (bounding box interactions like drag, select)\n",
        "        if sample.detections:\n",
        "            for det in sample.detections.detections:\n",
        "                # Extract detection attributes with safe defaults\n",
        "                task_desc = getattr(det, 'task_description', '')\n",
        "                element_info = getattr(det, 'element_info', None)\n",
        "                action = getattr(det, 'label', 'select')  # Default action is select\n",
        "                bounding_box = getattr(det, 'bounding_box', [])\n",
        "                custom_metadata = getattr(det, 'custom_metadata', None)\n",
        "                \n",
        "                # Ensure element_info is a proper dict or string\n",
        "                if element_info is None or element_info == {} or element_info == '':\n",
        "                    element_info = \"ui_region\"  # Simple string fallback\n",
        "                elif isinstance(element_info, dict) and not element_info:\n",
        "                    element_info = \"ui_region\"  # Empty dict -> string\n",
        "                elif not isinstance(element_info, (dict, str)):\n",
        "                    element_info = str(element_info)  # Convert to string if weird type\n",
        "                \n",
        "                # Ensure custom_metadata is a proper dict\n",
        "                if custom_metadata is None or custom_metadata == {} or custom_metadata == '':\n",
        "                    custom_metadata = {\"type\": \"bbox_interaction\"}\n",
        "                elif not isinstance(custom_metadata, dict):\n",
        "                    custom_metadata = {\"value\": str(custom_metadata)}\n",
        "                \n",
        "                # Only process if we have valid bounding box\n",
        "                if bounding_box and len(bounding_box) == 4:\n",
        "                    # FiftyOne format: [x, y, width, height] in relative coords [0,1]\n",
        "                    x, y, width, height = bounding_box\n",
        "                    \n",
        "                    # Convert to [x_min, y_min, x_max, y_max] for consistency\n",
        "                    x_min = round(x, 4)\n",
        "                    y_min = round(y, 4)\n",
        "                    x_max = round(x + width, 4)\n",
        "                    y_max = round(y + height, 4)\n",
        "                    bbox_gt_format = [x_min, y_min, x_max, y_max]\n",
        "                    \n",
        "                    # Build the JSON response object\n",
        "                    json_response = {\n",
        "                        \"action\": action,\n",
        "                        \"element_info\": element_info,\n",
        "                        \"bounding_box\": bbox_gt_format,\n",
        "                        \"custom_metadata\": custom_metadata\n",
        "                    }\n",
        "                    \n",
        "                    # Create natural language response with embedded JSON\n",
        "                    # Note: from_coord/to_coord format matches training patterns\n",
        "                    response_text = f\"\"\"I will {action} the {element_info if isinstance(element_info, str) else 'element'}. ```json {json.dumps(json_response)}```\"\"\"\n",
        "                    \n",
        "                    # Create the full conversation format\n",
        "                    messages = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": BB_SYSTEM_MESSAGE}\n",
        "                            ]\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"image\", \"image\": filepath},\n",
        "                                {\"type\": \"text\", \"text\": task_desc}\n",
        "                            ]\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": response_text}\n",
        "                            ],\n",
        "                            \"recipient\": \"os\",\n",
        "                            \"end_turn\": True,\n",
        "                            \"bbox_gt\": bbox_gt_format  # Ground truth for pointer loss\n",
        "                        }\n",
        "                    ]\n",
        "                    \n",
        "                    det.message_payload = messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFbeo6sT-loM",
        "outputId": "5ded4724-a861-4222-ede4-d1f7e12297f5"
      },
      "outputs": [],
      "source": [
        "add_message_payload_to_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2WXK7c2TAaE",
        "outputId": "dee09252-0a1a-4e8d-84a2-19889ea4ce39"
      },
      "outputs": [],
      "source": [
        "dataset.first().keypoints.keypoints[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUcuxPgnS-wF"
      },
      "source": [
        "# Split the dataset into training and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr2GaOyLTFS4"
      },
      "source": [
        "# Create PyTorch Datasets from the FiftyOne Dataset\n",
        "\n",
        "\n",
        "FiftyOne's dataset operations don't naturally align with PyTorch's training loops.\n",
        "\n",
        "The solution is creating a flattened dataset structure where each annotation becomes an independent sample, implementing proper worker initialization for FiftyOne's multiprocessing, preserving file paths through the transformation pipeline for image loading, handling both keypoint and detection annotations with different processing logic, and maintaining annotation metadata through to the training loop.\n",
        "\n",
        "This allows researchers to use FiftyOne's powerful dataset management while training GUI-Actor models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEDFe2hCpCMR"
      },
      "outputs": [],
      "source": [
        "from fiftyone.utils.torch import GetItem\n",
        "\n",
        "class DataGetter(GetItem):\n",
        "    @property\n",
        "    def required_keys(self):\n",
        "        return ['filepath', 'keypoints', 'detections']\n",
        "\n",
        "    def __call__(self, d):\n",
        "        message_payloads = []\n",
        "\n",
        "        # Extract message_payload from all keypoints in the sample\n",
        "        keypoints = d.get(\"keypoints\")\n",
        "        if keypoints is not None and hasattr(keypoints, 'keypoints'):\n",
        "            for keypoint in keypoints.keypoints:\n",
        "                if hasattr(keypoint, 'message_payload') and keypoint.message_payload is not None:\n",
        "                    message_payloads.append(keypoint.message_payload)\n",
        "\n",
        "        # Extract message_payload from all detections in the sample\n",
        "        detections = d.get(\"detections\")\n",
        "        if detections is not None and hasattr(detections, 'detections'):\n",
        "            for detection in detections.detections:\n",
        "                if hasattr(detection, 'message_payload') and detection.message_payload is not None:\n",
        "                    message_payloads.append(detection.message_payload)\n",
        "\n",
        "        return {\n",
        "            \"filepath\": d.get(\"filepath\", \"\"),\n",
        "            \"message_payload\": message_payloads,\n",
        "        }\n",
        "\n",
        "\n",
        "class FlattenedDataset:\n",
        "    \"\"\"\n",
        "    Flattens a FiftyOne torch dataset so each item is a single message_payload\n",
        "    with its associated filepath.\n",
        "    \"\"\"\n",
        "    def __init__(self, fiftyone_torch_dataset):\n",
        "        self.items = []\n",
        "        for sample in fiftyone_torch_dataset:\n",
        "            filepath = sample[\"filepath\"]\n",
        "            for message_payload in sample[\"message_payload\"]:\n",
        "                if message_payload:  # Only add non-empty payloads\n",
        "                    self.items.append({\n",
        "                        \"filepath\": filepath,\n",
        "                        \"message_payload\": message_payload\n",
        "                    })\n",
        "        print(f\"FlattenedDataset created with {len(self.items)} items\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGJ7yr4OGll5",
        "outputId": "65e908fa-93a2-47e5-869f-3817374a145a"
      },
      "outputs": [],
      "source": [
        "train_view = dataset.match_tags(\"train\")\n",
        "\n",
        "val_view = dataset.match_tags(\"val\")\n",
        "\n",
        "# Create torch datasets using DataGetter\n",
        "train_torch_dataset = train_view.to_torch(DataGetter())\n",
        "val_torch_dataset = val_view.to_torch(DataGetter())\n",
        "\n",
        "\n",
        "train_dataset = FlattenedDataset(train_torch_dataset)\n",
        "val_dataset = FlattenedDataset(val_torch_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZWJLmbRWRi2"
      },
      "source": [
        "## Behind the Scenes: Collation\n",
        "\n",
        "The collator became the heart of the system, orchestrating multiple processing stages for each batch.\n",
        "\n",
        "It extracts ground truth coordinates from message payloads, processes images through the vision encoder to get patch dimensions, reformats text to replace coordinates with special pointer tokens, calculates visual token indices for supervision, generates patch labels for region-based interactions, and handles padding and batching while preserving all this structured information.\n",
        "\n",
        "Each stage had to handle edge cases and validation to ensure training stability.\n",
        "\n",
        "## GUI-Actor Fine-Tuning Recipe\n",
        "\n",
        "The fine-tuning strategy for GUI-Actor on FiftyOne datasets has two distinct modes:\n",
        "\n",
        "### 1. **General Fine-tuning** (for multi-application scenarios)\n",
        "\n",
        "```python\n",
        "# Optimized for generalization across multiple applications\n",
        "num_train_epochs=3\n",
        "learning_rate=2e-5\n",
        "gradient_accumulation_steps=4\n",
        "warmup_ratio=0.1\n",
        "weight_decay=0.01\n",
        "unfreeze_all_parameters=False\n",
        "```\n",
        "\n",
        "### 2. **Single-Application Mode** (for specialized use cases)\n",
        "\n",
        "The implementation provides flexible parameter unfreezing strategies optimized for different training objectives.\n",
        "\n",
        "```python\n",
        "# Aggressive specialization for one target application\n",
        "single_app_mode=True  # Auto-configures aggressive settings\n",
        "unfreeze_vision_layers=True  # (auto-enabled)\n",
        "unfreeze_last_n_layers=8  # (auto-increased from 4)\n",
        "# Allows intentional overfitting for maximum specialization\n",
        "```\n",
        "\n",
        "## 🧠 Parameter Unfreezing Strategy\n",
        "\n",
        "### Layer-wise Unfreezing Hierarchy:\n",
        "\n",
        "| Component | Default Mode | Single-App Mode | Rationale |\n",
        "|-----------|--------------|-----------------|-----------|\n",
        "| **Vision Layers** | ❌ Frozen | ✅ Last 25% unfrozen | Single-app needs visual specialization |\n",
        "| **Language Layers** | ✅ Last 4 unfrozen | ✅ Last 8+ unfrozen | Task-specific reasoning adaptation |\n",
        "| **LM Head** | ✅ Always unfrozen | ✅ Always unfrozen | Output vocabulary adaptation |\n",
        "| **Pointer Head** | ✅ Always unfrozen | ✅ Always unfrozen | Coordinate prediction adaptation |\n",
        "\n",
        "## 📊 Training Hyperparameters\n",
        "\n",
        "### Learning Rate Strategy\n",
        "\n",
        "- **Base LR**: `2e-5` (higher than original `5e-6` for better fine-tuning)\n",
        "- **Warmup**: `10%` of total steps (vs original `3%`)\n",
        "- **Scheduler**: Cosine decay for smooth convergence\n",
        "- **Weight Decay**: `0.01` for regularization (vs original `0.0`)\n",
        "\n",
        "### Batch Configuration\n",
        "- **Train Batch Size**: `1` per device (memory optimized)\n",
        "- **Eval Batch Size**: `4` per device (faster evaluation)\n",
        "- **Gradient Accumulation**: `4` steps (effective batch size = 4)\n",
        "- **Workers**: `4` (reduced from `8` for stability)\n",
        "\n",
        "### Training Duration\n",
        "- **Epochs**: `3` (vs original `1`) for better convergence\n",
        "- **Save Frequency**: Every `500` steps (vs original `2000`)\n",
        "- **Evaluation**: Enabled with validation set monitoring\n",
        "\n",
        "## Usage Recommendations\n",
        "\n",
        "### For General Fine-tuning (Multi-application)\n",
        "```bash\n",
        "python train.py --dataset_name your_dataset\n",
        "# Uses conservative unfreezing, good generalization\n",
        "```\n",
        "\n",
        "### For Single-Application Specialization\n",
        "```bash\n",
        "python train.py --dataset_name your_app_data --single_app_mode\n",
        "# Aggressive unfreezing, maximum specialization\n",
        "```\n",
        "\n",
        "### For Conservative Fine-tuning (Small datasets)\n",
        "```bash\n",
        "python train.py --dataset_name small_dataset --unfreeze_last_n_layers 2\n",
        "# Minimal unfreezing, prevents overfitting\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839,
          "referenced_widgets": [
            "4d4d91d6a1ff4e1f8e8841d2a14b5bc0",
            "709596e5027c4823b12c81771412d18b",
            "138cf512c2954f249f2000b702cf9517",
            "c2b17dfc33664ebbae4737a879411cfd",
            "b10b0e64b3ab45538bfda2ae9fa891bc",
            "2c75a7aa89364c0c97768808ce357221",
            "410dbac264cc415fae744b8a390a1ce6",
            "7e78f61f60934f78b0c2a08cb2d9eece",
            "ccee604699d94827ae590d1963365431",
            "abfe3cdb20964ff5b93090073adf7bab",
            "2ab4b34503c64f5facc888910d60c877"
          ]
        },
        "id": "usY22fFpdZ-d",
        "outputId": "fc0d1f64-9df4-4b65-afd1-8e11c6c0e240"
      },
      "outputs": [],
      "source": [
        "from gui_actor.train import train_gui_actor_on_fiftyone\n",
        "\n",
        "# Train the model\n",
        "model, processor = train_gui_actor_on_fiftyone(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    num_train_epochs=1,\n",
        "    single_app_mode=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geui7WnBGaNX"
      },
      "outputs": [],
      "source": [
        "# One-liner to push to HF Hub (replace with your repo name)\n",
        "model.push_to_hub(\"gui-actor-fiftyone-finetuned-on-synthetic\", private=True)\n",
        "processor.push_to_hub(\"gui-actor-fiftyone-finetuned-on-synthetic\", private=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "138cf512c2954f249f2000b702cf9517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e78f61f60934f78b0c2a08cb2d9eece",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccee604699d94827ae590d1963365431",
            "value": 2
          }
        },
        "2ab4b34503c64f5facc888910d60c877": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c75a7aa89364c0c97768808ce357221": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "410dbac264cc415fae744b8a390a1ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d4d91d6a1ff4e1f8e8841d2a14b5bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_709596e5027c4823b12c81771412d18b",
              "IPY_MODEL_138cf512c2954f249f2000b702cf9517",
              "IPY_MODEL_c2b17dfc33664ebbae4737a879411cfd"
            ],
            "layout": "IPY_MODEL_b10b0e64b3ab45538bfda2ae9fa891bc"
          }
        },
        "709596e5027c4823b12c81771412d18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c75a7aa89364c0c97768808ce357221",
            "placeholder": "​",
            "style": "IPY_MODEL_410dbac264cc415fae744b8a390a1ce6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7e78f61f60934f78b0c2a08cb2d9eece": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abfe3cdb20964ff5b93090073adf7bab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b10b0e64b3ab45538bfda2ae9fa891bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b17dfc33664ebbae4737a879411cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abfe3cdb20964ff5b93090073adf7bab",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab4b34503c64f5facc888910d60c877",
            "value": " 2/2 [00:02&lt;00:00,  1.29s/it]"
          }
        },
        "ccee604699d94827ae590d1963365431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
