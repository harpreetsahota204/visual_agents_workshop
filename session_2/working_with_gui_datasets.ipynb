{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e9c9e",
   "metadata": {},
   "source": [
    "We'll be making use a couple of plugins to help us with our analysis. Let's go ahead and install it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22dd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/synthetic_gui_samples_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/ehofesmann/edit_label_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84039a85",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "In this section, we'll download a dataset from Google Drive using `gdown`. The file is hosted at this URL:\n",
    "https://drive.google.com/file/d/1YEhdlXuTV_nalVvKuXSHZa_JgkuIXjcG/view?usp=sharing\n",
    "\n",
    "After downloading, we'll extract its contents to work with the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18108fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Download the file\n",
    "url = \"https://drive.google.com/uc?id=1YEhdlXuTV_nalVvKuXSHZa_JgkuIXjcG\"\n",
    "gdown.download(url, \"data.zip\", quiet=False)\n",
    "\n",
    "# Extract the contents\n",
    "with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f13267",
   "metadata": {},
   "source": [
    "## Loading Annotations\n",
    "\n",
    "Now we'll load and explore the COCO4GUI annotations from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b06c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# Load the annotations\n",
    "annotations_path = \"./data/annotations_coco.json\"\n",
    "with open(annotations_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Print basic info about the dataset\n",
    "print(f\"Keys in the annotations file: {list(annotations.keys())}\")\n",
    "print(f\"Number of images: {len(annotations['images'])}\")\n",
    "print(f\"Number of annotations: {len(annotations['annotations'])}\")\n",
    "print(f\"Categories: {[cat['name'] for cat in annotations['categories']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Select a random image and its annotations\n",
    "random_image = random.choice(annotations['images'])\n",
    "\n",
    "image_id = random_image['id']\n",
    "\n",
    "# Get annotations for this image\n",
    "image_annotations = [a for a in annotations['annotations'] if a['image_id'] == image_id]\n",
    "\n",
    "# Print image info and its annotations\n",
    "print(f\"Selected image: {random_image['file_name']}\")\n",
    "\n",
    "Image.open(f\"./data/{random_image['file_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5432dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image ID: {image_id}\")\n",
    "print(f\"Image size: {random_image['width']} x {random_image['height']}\")\n",
    "print(f\"Number of annotations for this image: {len(image_annotations)}\")\n",
    "\n",
    "# Print one annotation as an example\n",
    "if image_annotations:\n",
    "    print(\"\\nExample annotation:\")\n",
    "    pprint(image_annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9da2ce",
   "metadata": {},
   "source": [
    "## Loading Data into FiftyOne\n",
    "\n",
    "[FiftyOne](https://voxel51.com/fiftyone/) is a powerful tool for visualizing, exploring and analyzing image datasets. We'll use a specialized COCO4GUI format to load our GUI interaction dataset into FiftyOne.\n",
    "\n",
    "The [COCO4GUI](https://github.com/harpreetsahota204/coco4gui_fiftyone) format extends the standard COCO detection format to handle GUI-specific features:\n",
    "\n",
    "- **Dual annotation support**: Both bounding boxes for UI elements and keypoints for interaction points\n",
    "\n",
    "- **Sequence information**: Tracking user workflows and interaction chains\n",
    "\n",
    "- **GUI metadata**: Application, platform, and timing information\n",
    "\n",
    "- **Rich attributes**: Task descriptions, element information, and custom metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472534b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/harpreetsahota204/coco4gui_fiftyone/main/coco4gui.py -o coco4gui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231353df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from coco4gui import COCO4GUIDataset\n",
    "\n",
    "# Define paths to our dataset\n",
    "dataset_dir = \"/Users/harpreetsahota/workspace/visual_agents_workshop/session_2\"\n",
    "data_path = \"data\"  # Image directory\n",
    "labels_path = \"data/annotations_coco.json\"  # COCO annotations file\n",
    "\n",
    "# Create the dataset\n",
    "gui_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=COCO4GUIDataset,\n",
    "    name=\"session_2_dataset\",\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    overwrite=True,\n",
    "    persistent=True,\n",
    "    include_sequence_info=False,  # Extract sequence info\n",
    "    include_gui_metadata=True,   # Include GUI metadata\n",
    "    extra_attrs=True,            # Include all attributes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94ae62",
   "metadata": {},
   "source": [
    "Let's see what a sample looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b12d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gui_dataset.skip(51).first()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accessing all detections for a sample\n",
    "print(\"All detections for this sample:\")\n",
    "print(sample.detections.detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54267938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Accessing a specific detection by index\n",
    "print(\"\\nFirst detection:\")\n",
    "print(sample.detections.detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filtering detections by label\n",
    "hover_detections = [d for d in sample.detections.detections if d.label == \"hover\"]\n",
    "\n",
    "print(f\"\\nFound {len(hover_detections)} hover detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c65add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Accessing specific attributes of a detection\n",
    "if hover_detections:\n",
    "    detection = hover_detections[0]\n",
    "    print(\"\\nDetection details:\")\n",
    "    print(f\"- Label: {detection.label}\")\n",
    "    print(f\"- Bounding box: {detection.bounding_box}\")\n",
    "    print(f\"- Task description: {detection.task_description}\")\n",
    "    \n",
    "    # Accessing nested attributes\n",
    "    if hasattr(detection, \"custom_metadata\") and detection.custom_metadata:\n",
    "        print(f\"- Custom metadata value: {detection.custom_metadata['value']}\")\n",
    "\n",
    "# Note: FiftyOne allows you to access labels as Python objects\n",
    "# with dot notation, making it easy to navigate complex label structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510a3fc",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "With the dataset loaded in FiftyOne, you can:\n",
    "\n",
    "1. **Visualize annotations**: See both bounding boxes and keypoints in the same view\n",
    "\n",
    "2. **Filter interactions**: Use the query interface to find specific interaction types like clicks or drags\n",
    "\n",
    "3. **Explore sequences**: View related interactions in a workflow sequence\n",
    "\n",
    "4. **View metadata**: See application info, platform details, and custom attributes\n",
    "\n",
    "5. **Create views**: Focus on subsets of data for detailed analysis\n",
    "\n",
    "Try these example queries in the FiftyOne App:\n",
    "\n",
    "- `F(\"ground_truth.detections.label\") == \"click\"` - Show only click interactions\n",
    "\n",
    "- `F(\"application\") == \"Chrome\"` - Filter by application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating filtered views of the dataset\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Create a view with only click interactions\n",
    "clicks = gui_dataset.filter_labels(\"detections\", F(\"label\") == \"click\")\n",
    "\n",
    "gui_dataset.save_view(\"click_boxes\", clicks)\n",
    "\n",
    "print(f\"Click interactions (bounding boxes): {len(clicks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53213b45",
   "metadata": {},
   "source": [
    "## Identifying and Organizing Incomplete Annotations\n",
    "\n",
    "We can easily create views that identify and organize samples with missing annotation data.  These views help us:\n",
    "\n",
    "1. Quickly find UI elements with incomplete annotations that need attention\n",
    "   \n",
    "2. Focus annotation resources on specific types of missing data\n",
    "   \n",
    "3. Monitor overall annotation quality as the dataset evolves\n",
    "   \n",
    "\n",
    "Saving these views creates shortcuts to these filtered datasets that can be accessed through the UI or programmatically, making dataset curation much more efficient.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Notice we are using `filter_labels` to filter the labels within each sample. \n",
    "\n",
    "This filters the contents of label fields within each sample. \n",
    "\n",
    "It removes or keeps only the labels that match a given filter, but all samples remain in the view (unless you set `only_matches=True`, which will exclude samples with no matching labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ae15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Create a view with missing task descriptions\n",
    "no_bb_tasks = gui_dataset.filter_labels(\"detections\", F(\"task_description\") == \"\")\n",
    "\n",
    "gui_dataset.save_view(\"no_bb_tasks\", no_bb_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Create a view with missing task descriptions\n",
    "no_bb_tasks_w_match = gui_dataset.filter_labels(\"detections\", F(\"task_description\") == \"\", only_matches=True)\n",
    "\n",
    "gui_dataset.save_view(\"no_bb_tasks_w_match\", no_bb_tasks_w_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eaa66c",
   "metadata": {},
   "source": [
    "We can also use `match_labels`, which selects samples that contain at least one label matching the specified criteria. \n",
    "\n",
    "It does not filter the labels themselves; instead, it includes or excludes entire samples based on whether they have a matching label. For example, to keep only samples that have at least one detection with missing action types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d450f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view with missing action types\n",
    "\n",
    "no_bb_action = gui_dataset.match_labels(\"detections\", F(\"action_type\") == \" \")\n",
    "\n",
    "gui_dataset.save_view(\"no_bb_action\", no_bb_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4e9cd",
   "metadata": {},
   "source": [
    "This keeps only the samples with at least one matching label, but does not alter the labels within those samples.  [Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html), [API Reference](https://docs.voxel51.com/api/fiftyone.core.collections.html)\n",
    "\n",
    "\n",
    "\n",
    "**When to use which:**\n",
    "\n",
    "- Use `filter_labels` when you want to keep all samples but only certain labels within them. That is, you want to **filter labels within samples**. \n",
    "\n",
    "- Use `match_labels` when you want to keep only those samples that have at least one label matching your criteria. That is, you want to **filters samples based on their labels**.\n",
    "\n",
    "For more details and examples, see the [Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html) and [Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view with missing labels\n",
    "\n",
    "no_bb_labels = gui_dataset.filter_labels(\"detections\", F(\"label\") == \"\")\n",
    "\n",
    "gui_dataset.save_view(\"no_bb_labels\", no_bb_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815caa4",
   "metadata": {},
   "source": [
    "You can also do the same with keypoints as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "305b4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view with missing labels\n",
    "\n",
    "no_kp_tasks = gui_dataset.filter_labels(\"keypoints\", F(\"task_description\") == \"\")\n",
    "\n",
    "gui_dataset.save_view(\"no_kp_tasks\", no_kp_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d58bdd",
   "metadata": {},
   "source": [
    "## Analyzing Bounding Box Areas\n",
    "\n",
    "To compute the area of bounding boxes in FiftyOne, you can use a [ViewExpression]((https://docs.voxel51.com/api/fiftyone.core.expressions.html).). Bounding boxes are stored in the format [top-left-x, top-left-y, width, height], where width and height are relative to the image size (values between 0 and 1).\n",
    "\n",
    "This computes the area as width × height in relative coordinates (as a fraction of the image).\n",
    "\n",
    "You can use these expressions in view stages like `filter_labels` to filter detections by area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218d5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "gui_dataset.set_field(\"detections.detections.relative_bbox_area\", rel_bbox_area).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94743ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SampleView: {\n",
       "    'id': '68a89a1de619a3e80badb873',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/visual_agents_workshop/session_2/data/2025-07-24_17-20-32.png',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 1064,\n",
       "        'height': 1080,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 8, 22, 16, 26, 5, 464000),\n",
       "    'last_modified_at': datetime.datetime(2025, 8, 22, 16, 34, 57, 397000),\n",
       "    'keypoints': <Keypoints: {\n",
       "        'keypoints': [\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb840',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.04077669902912621, 0.4660212320574163]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear the prompt field from being shown in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Checkbox',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb841',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.038834951456310676, 0.4995140550239235]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear the image_source field from being shown on the samples in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Checkbox',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb842',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.26796116504854367, 0.3119542464114833]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear all primitives fields from being shown in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Icon',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'application': 'Arc Browser',\n",
       "    'platform': 'macOS',\n",
       "    'detections': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '68a89a1de619a3e80badb83e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'hover',\n",
       "                'bounding_box': [\n",
       "                    0.44174757281553395,\n",
       "                    0.2937724282296651,\n",
       "                    0.10776699029126213,\n",
       "                    0.10813397129186604,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'What is the value of image_source associated with this sample?',\n",
       "                'action_type': 'hover',\n",
       "                'element_info': 'Text',\n",
       "                'custom_metadata': {\n",
       "                    'value': 'The image_source of this sample is midjourney. We can tell this because the background color of the text corresponds to the same color as image_source',\n",
       "                },\n",
       "                'relative_bbox_area': 0.011653272634366146,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gui_dataset.skip(51).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ee62a",
   "metadata": {},
   "source": [
    "## Counting UI Elements and Interaction Points\n",
    "\n",
    "We can also easily store counts of UI elements and interaction points for each sample in our GUI dataset. \n",
    "\n",
    "This makes it easier to analyze the complexity and interaction density of different interfaces.\n",
    "\n",
    "`ViewField (F)` provides a powerful way to extract and manipulate data from our samples. \n",
    "\n",
    "We're using the `.length()` method to count the number of detections and keypoints in each sample, then storing these counts as new fields in our dataset.\n",
    "\n",
    "These count fields enable us to:\n",
    "\n",
    "- Filter samples by UI complexity (e.g., find screens with many UI elements)\n",
    "\n",
    "- Analyze the relationship between UI elements and interaction points\n",
    "\n",
    "- Create visualizations based on interaction density\n",
    "\n",
    "- Sort and group samples by complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b82b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "box_counts = gui_dataset.values(F(\"detections.detections\").length())\n",
    "\n",
    "gui_dataset.set_values(\"box_counts\", box_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9110460",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_counts = gui_dataset.values(F(\"keypoints.keypoints\").length())\n",
    "\n",
    "gui_dataset.set_values(\"point_counts\", point_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07fd3fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SampleView: {\n",
       "    'id': '68a89a1de619a3e80badb873',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/visual_agents_workshop/session_2/data/2025-07-24_17-20-32.png',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 1064,\n",
       "        'height': 1080,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 8, 22, 16, 26, 5, 464000),\n",
       "    'last_modified_at': datetime.datetime(2025, 8, 22, 16, 35, 36, 122000),\n",
       "    'keypoints': <Keypoints: {\n",
       "        'keypoints': [\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb840',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.04077669902912621, 0.4660212320574163]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear the prompt field from being shown in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Checkbox',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb841',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.038834951456310676, 0.4995140550239235]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear the image_source field from being shown on the samples in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Checkbox',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "            <Keypoint: {\n",
       "                'id': '68a89a1de619a3e80badb842',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'click',\n",
       "                'points': [[0.26796116504854367, 0.3119542464114833]],\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'visible': [2],\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'Clear all primitives fields from being shown in the samples panel',\n",
       "                'action_type': 'click',\n",
       "                'element_info': 'Icon',\n",
       "                'custom_metadata': {},\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'application': 'Arc Browser',\n",
       "    'platform': 'macOS',\n",
       "    'detections': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '68a89a1de619a3e80badb83e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'hover',\n",
       "                'bounding_box': [\n",
       "                    0.44174757281553395,\n",
       "                    0.2937724282296651,\n",
       "                    0.10776699029126213,\n",
       "                    0.10813397129186604,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'interaction',\n",
       "                'iscrowd': 0,\n",
       "                'task_description': 'What is the value of image_source associated with this sample?',\n",
       "                'action_type': 'hover',\n",
       "                'element_info': 'Text',\n",
       "                'custom_metadata': {\n",
       "                    'value': 'The image_source of this sample is midjourney. We can tell this because the background color of the text corresponds to the same color as image_source',\n",
       "                },\n",
       "                'relative_bbox_area': 0.011653272634366146,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'box_counts': 1,\n",
       "    'point_counts': 3,\n",
       "}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gui_dataset.skip(51).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074c79a",
   "metadata": {},
   "source": [
    "## Advanced Embedding Analysis with FiftyOne\n",
    "\n",
    "FiftyOne's Brain module provides powerful capabilities for working with embeddings. \n",
    "\n",
    "Here are some of the most powerful things you can do:\n",
    "\n",
    "#### Visualizing Embeddings with Dimensionality Reduction\n",
    "\n",
    "You can project high-dimensional embeddings into 2D or 3D space using techniques like:\n",
    "- **UMAP**: Non-linear dimensionality reduction that preserves local relationships\n",
    "- **t-SNE**: Excellent for visualizing clusters in high-dimensional data\n",
    "- **PCA**: Linear dimensionality reduction that captures maximum variance\n",
    "\n",
    "This visualization allows you to:\n",
    "- Discover patterns and clusters in your GUI dataset\n",
    "- Identify outliers or unusual interactions\n",
    "- Compare different interaction types visually\n",
    "- Explore the feature space of your data\n",
    "\n",
    "\n",
    "Here's how to compute embeddings for GUI screenshots. We'll use a pretrained model from the FiftyOne Model Zoo. For actual GUI analysis, you might want a model trained on UI elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99426e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.utils:Computing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 739/739 [23.8s elapsed, 0s remaining, 18.9 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 739/739 [23.8s elapsed, 0s remaining, 18.9 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.visualization:Generating visualization...\n",
      "/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Fri Aug 22 11:36:56 2025 Construct fuzzy simplicial set\n",
      "Fri Aug 22 11:36:57 2025 Finding Nearest Neighbors\n",
      "Fri Aug 22 11:36:59 2025 Finished Nearest Neighbor Search\n",
      "Fri Aug 22 11:36:59 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114dfcfd063498fb85f547c9de32c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Fri Aug 22 11:37:00 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\"open-clip-torch\")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    gui_dataset,\n",
    "    model=clip_model,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",  # \"umap\", \"tsne\", \"pca\", etc\n",
    "    brain_key=\"clip_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478814a4",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Similarity Search\n",
    "\n",
    "Build powerful similarity indexes for:\n",
    "- Finding visually similar GUI screens\n",
    "- Identifying related interaction patterns\n",
    "- Retrieving nearest neighbors for any sample\n",
    "\n",
    "FiftyOne supports multiple backends including:\n",
    "- Sklearn (default, in-memory)\n",
    "- Vector databases (Qdrant, Pinecone, Redis, Milvus, etc.)\n",
    "- Document stores (MongoDB, Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68bb01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_similarity(\n",
    "    gui_dataset,\n",
    "    backend=\"sklearn\",  # Fast sklearn backend\n",
    "    brain_key=\"clip_sim\", \n",
    "    embeddings=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8a3ad",
   "metadata": {},
   "source": [
    "\n",
    "### Computing Representativeness\n",
    "\n",
    "Create diverse subsets of your GUI dataset by:\n",
    "- Selecting samples that best represent the entire distribution\n",
    "- Ensuring coverage of all interaction types and patterns\n",
    "- Avoiding redundancy in your selected examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a913dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving embeddings from similarity index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.utils:Retrieving embeddings from similarity index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing representativeness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Computing representativeness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing clusters for 739 embeddings; this may take awhile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Computing clusters for 739 embeddings; this may take awhile...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representativeness computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.brain.internal.core.representativeness:Representativeness computation complete\n"
     ]
    }
   ],
   "source": [
    "results = fob.compute_representativeness(\n",
    "    gui_dataset,\n",
    "    similarity_index=\"clip_sim\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e39dd5",
   "metadata": {},
   "source": [
    "# Patch views in FiftyOne\n",
    " \n",
    "Patch views are a powerful feature in FiftyOne that allow you to:\n",
    "- Extract regions of interest (like bounding boxes) as standalone samples\n",
    "- Work with these regions directly instead of the full images\n",
    "- Maintain all metadata and annotations from the parent sample\n",
    "- Perform analysis on specific UI elements rather than entire screens\n",
    "\n",
    "When we convert detections to patches (as in the next cell), each bounding box becomes its own sample with all associated metadata like task descriptions,action types, and element information preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_patches = gui_dataset.to_patches(\"detections\")\n",
    "box_patches=box_patches.clone(name=\"box_patches\")\n",
    "box_patches.persistent=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169be266",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_patches.skip(51).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0338fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device=\"mps\")\n",
    "\n",
    "# Iterate through patches and compute embeddings\n",
    "for patch in tqdm(box_patches, desc=\"Computing embeddings\"):\n",
    "    # Construct prompt from the three fields\n",
    "    task_desc = patch.detections.task_description if patch.detections.task_description else \"\"\n",
    "    action_type = patch.detections.action_type if patch.detections.action_type else \"\"\n",
    "    element_info = patch.detections.element_info if patch.detections.element_info else \"\"\n",
    "    \n",
    "    # Combine into prompt\n",
    "    prompt = f\"clustering: Task: {task_desc}; Action: {action_type}; Element: {element_info}\"\n",
    "    \n",
    "    # Compute embedding\n",
    "    embedding = model.encode(prompt)\n",
    "    \n",
    "    # Add as new field\n",
    "    patch['text_embedding'] = embedding.tolist()\n",
    "    patch.save()\n",
    "\n",
    "print(\"Done! Embeddings added to all patches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob \n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    box_patches,\n",
    "    embeddings=\"text_embedding\",\n",
    "    method=\"umap\",  # \"umap\", \"tsne\", \"pca\", etc\n",
    "    brain_key=\"text_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48495d0d",
   "metadata": {},
   "source": [
    "Let's get more hands on with the FiftyOne app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(gui_dataset)\n",
    "\n",
    "# or run this in the terminal: fiftyone app launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eba058",
   "metadata": {},
   "source": [
    "Now, we can push our dataset to the Hugging Face Hub.\n",
    "\n",
    "You will need a Hugging Face token, learn how you can get one [here](https://huggingface.co/docs/hub/en/security-tokens#how-to-manage-user-access-tokens).\n",
    "\n",
    "You'll then need to login to Hugging Face, which you can by entering the following in your terminal:\n",
    "\n",
    "`hf auth login`\n",
    "\n",
    "And then paste your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fff4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "fouh.push_to_hub(\n",
    "    gui_dataset,\n",
    "    \"FiftyOne-GUI-Grounding-Train\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
