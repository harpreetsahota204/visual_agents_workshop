{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e9c9e",
   "metadata": {},
   "source": [
    "We'll be making use a couple of plugins to help us with our analysis. Let's go ahead and install it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22dd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/synthetic_gui_samples_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/ehofesmann/edit_label_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84039a85",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "In this section, we'll download a dataset from Google Drive using `gdown`. The file is hosted at this URL:\n",
    "https://drive.google.com/file/d/1YEhdlXuTV_nalVvKuXSHZa_JgkuIXjcG/view?usp=sharing\n",
    "\n",
    "After downloading, we'll extract its contents to work with the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18108fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Download the file\n",
    "url = \"https://drive.google.com/uc?id=1YEhdlXuTV_nalVvKuXSHZa_JgkuIXjcG\"\n",
    "gdown.download(url, \"data.zip\", quiet=False)\n",
    "\n",
    "# Extract the contents\n",
    "with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f13267",
   "metadata": {},
   "source": [
    "## Loading Annotations\n",
    "\n",
    "Now we'll load and explore the COCO4GUI annotations from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b06c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# Load the annotations\n",
    "annotations_path = \"./data/annotations_coco.json\"\n",
    "with open(annotations_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Print basic info about the dataset\n",
    "print(f\"Keys in the annotations file: {list(annotations.keys())}\")\n",
    "print(f\"Number of images: {len(annotations['images'])}\")\n",
    "print(f\"Number of annotations: {len(annotations['annotations'])}\")\n",
    "print(f\"Categories: {[cat['name'] for cat in annotations['categories']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Select a random image and its annotations\n",
    "random_image = random.choice(annotations['images'])\n",
    "\n",
    "image_id = random_image['id']\n",
    "\n",
    "# Get annotations for this image\n",
    "image_annotations = [a for a in annotations['annotations'] if a['image_id'] == image_id]\n",
    "\n",
    "# Print image info and its annotations\n",
    "print(f\"Selected image: {random_image['file_name']}\")\n",
    "\n",
    "Image.open(f\"./data/{random_image['file_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5432dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image ID: {image_id}\")\n",
    "print(f\"Image size: {random_image['width']} x {random_image['height']}\")\n",
    "print(f\"Number of annotations for this image: {len(image_annotations)}\")\n",
    "\n",
    "# Print one annotation as an example\n",
    "if image_annotations:\n",
    "    print(\"\\nExample annotation:\")\n",
    "    pprint(image_annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9da2ce",
   "metadata": {},
   "source": [
    "## Loading Data into FiftyOne\n",
    "\n",
    "[FiftyOne](https://voxel51.com/fiftyone/) is a powerful tool for visualizing, exploring and analyzing image datasets. We'll use a specialized COCO4GUI format to load our GUI interaction dataset into FiftyOne.\n",
    "\n",
    "The [COCO4GUI](https://github.com/harpreetsahota204/coco4gui_fiftyone) format extends the standard COCO detection format to handle GUI-specific features:\n",
    "\n",
    "- **Dual annotation support**: Both bounding boxes for UI elements and keypoints for interaction points\n",
    "\n",
    "- **Sequence information**: Tracking user workflows and interaction chains\n",
    "\n",
    "- **GUI metadata**: Application, platform, and timing information\n",
    "\n",
    "- **Rich attributes**: Task descriptions, element information, and custom metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472534b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/harpreetsahota204/coco4gui_fiftyone/main/coco4gui.py -o coco4gui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231353df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from coco4gui import COCO4GUIDataset\n",
    "\n",
    "# Define paths to our dataset\n",
    "dataset_dir = \"/Users/harpreetsahota/workspace/visual_agents_workshop/session_2\"\n",
    "data_path = \"data\"  # Image directory\n",
    "labels_path = \"data/annotations_coco.json\"  # COCO annotations file\n",
    "\n",
    "# Create the dataset\n",
    "gui_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=COCO4GUIDataset,\n",
    "    name=\"session_2_dataset\",\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    persistent=True,\n",
    "    include_sequence_info=False,  # Extract sequence info\n",
    "    include_gui_metadata=True,   # Include GUI metadata\n",
    "    extra_attrs=True,            # Include all attributes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94ae62",
   "metadata": {},
   "source": [
    "Let's see what a sample looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b12d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gui_dataset.skip(51).first()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accessing all detections for a sample\n",
    "print(\"All detections for this sample:\")\n",
    "print(sample.detections.detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54267938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Accessing a specific detection by index\n",
    "print(\"\\nFirst detection:\")\n",
    "print(sample.detections.detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Filtering detections by label\n",
    "hover_detections = [d for d in sample.detections.detections if d.label == \"hover\"]\n",
    "\n",
    "print(f\"\\nFound {len(hover_detections)} hover detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c65add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Accessing specific attributes of a detection\n",
    "if hover_detections:\n",
    "    detection = hover_detections[0]\n",
    "    print(\"\\nDetection details:\")\n",
    "    print(f\"- Label: {detection.label}\")\n",
    "    print(f\"- Bounding box: {detection.bounding_box}\")\n",
    "    print(f\"- Task description: {detection.task_description}\")\n",
    "    \n",
    "    # Accessing nested attributes\n",
    "    if hasattr(detection, \"custom_metadata\") and detection.custom_metadata:\n",
    "        print(f\"- Custom metadata value: {detection.custom_metadata['value']}\")\n",
    "\n",
    "# Note: FiftyOne allows you to access labels as Python objects\n",
    "# with dot notation, making it easy to navigate complex label structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510a3fc",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "With the dataset loaded in FiftyOne, you can:\n",
    "\n",
    "1. **Visualize annotations**: See both bounding boxes and keypoints in the same view\n",
    "\n",
    "2. **Filter interactions**: Use the query interface to find specific interaction types like clicks or drags\n",
    "\n",
    "3. **Explore sequences**: View related interactions in a workflow sequence\n",
    "\n",
    "4. **View metadata**: See application info, platform details, and custom attributes\n",
    "\n",
    "5. **Create views**: Focus on subsets of data for detailed analysis\n",
    "\n",
    "Try these example queries in the FiftyOne App:\n",
    "\n",
    "- `F(\"ground_truth.detections.label\") == \"click\"` - Show only click interactions\n",
    "\n",
    "- `F(\"application\") == \"Chrome\"` - Filter by application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating filtered views of the dataset\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Create a view with only click interactions\n",
    "clicks = gui_dataset.filter_labels(\"detections\", F(\"label\") == \"click\")\n",
    "\n",
    "gui_dataset.save_view(\"click_boxes\", clicks)\n",
    "\n",
    "print(f\"Click interactions (bounding boxes): {len(clicks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ae15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view with missing task descriptions\n",
    "\n",
    "no_tasks = gui_dataset.filter_labels(\"detections\", F(\"task_description\") == \"\")\n",
    "\n",
    "gui_dataset.save_view(\"no_tasks\", no_tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d58bdd",
   "metadata": {},
   "source": [
    "## Analyzing Bounding Box Areas\n",
    "\n",
    "To compute the area of bounding boxes in FiftyOne, you can use a [ViewExpression]((https://docs.voxel51.com/api/fiftyone.core.expressions.html).). Bounding boxes are stored in the format [top-left-x, top-left-y, width, height], where width and height are relative to the image size (values between 0 and 1).\n",
    "\n",
    "This computes the area as width × height in relative coordinates (as a fraction of the image).\n",
    "\n",
    "You can use these expressions in view stages like `filter_labels` to filter detections by area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "gui_dataset.set_field(\"detections.detections.relative_bbox_area\", rel_bbox_area).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94743ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui_dataset.skip(51).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074c79a",
   "metadata": {},
   "source": [
    "## Advanced Embedding Analysis with FiftyOne\n",
    "\n",
    "FiftyOne's Brain module provides powerful capabilities for working with embeddings. \n",
    "\n",
    "Here are some of the most powerful things you can do:\n",
    "\n",
    "#### Visualizing Embeddings with Dimensionality Reduction\n",
    "\n",
    "You can project high-dimensional embeddings into 2D or 3D space using techniques like:\n",
    "- **UMAP**: Non-linear dimensionality reduction that preserves local relationships\n",
    "- **t-SNE**: Excellent for visualizing clusters in high-dimensional data\n",
    "- **PCA**: Linear dimensionality reduction that captures maximum variance\n",
    "\n",
    "This visualization allows you to:\n",
    "- Discover patterns and clusters in your GUI dataset\n",
    "- Identify outliers or unusual interactions\n",
    "- Compare different interaction types visually\n",
    "- Explore the feature space of your data\n",
    "\n",
    "\n",
    "Here's how to compute embeddings for GUI screenshots. We'll use a pretrained model from the FiftyOne Model Zoo. For actual GUI analysis, you might want a model trained on UI elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\"open-clip-torch\")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    gui_dataset,\n",
    "    model=clip_model,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",  # \"umap\", \"tsne\", \"pca\", etc\n",
    "    brain_key=\"clip_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478814a4",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Similarity Search\n",
    "\n",
    "Build powerful similarity indexes for:\n",
    "- Finding visually similar GUI screens\n",
    "- Identifying related interaction patterns\n",
    "- Retrieving nearest neighbors for any sample\n",
    "\n",
    "FiftyOne supports multiple backends including:\n",
    "- Sklearn (default, in-memory)\n",
    "- Vector databases (Qdrant, Pinecone, Redis, Milvus, etc.)\n",
    "- Document stores (MongoDB, Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_similarity(\n",
    "    gui_dataset,\n",
    "    backend=\"sklearn\",  # Fast sklearn backend\n",
    "    brain_key=\"clip_sim\", \n",
    "    embeddings=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8a3ad",
   "metadata": {},
   "source": [
    "\n",
    "### Computing Representativeness\n",
    "\n",
    "Create diverse subsets of your GUI dataset by:\n",
    "- Selecting samples that best represent the entire distribution\n",
    "- Ensuring coverage of all interaction types and patterns\n",
    "- Avoiding redundancy in your selected examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_representativeness(\n",
    "    gui_dataset,\n",
    "    similarity_index=\"clip_sim\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e39dd5",
   "metadata": {},
   "source": [
    "# Patch views in FiftyOne\n",
    " \n",
    "Patch views are a powerful feature in FiftyOne that allow you to:\n",
    "- Extract regions of interest (like bounding boxes) as standalone samples\n",
    "- Work with these regions directly instead of the full images\n",
    "- Maintain all metadata and annotations from the parent sample\n",
    "- Perform analysis on specific UI elements rather than entire screens\n",
    "\n",
    "When we convert detections to patches (as in the next cell), each bounding box becomes its own sample with all associated metadata like task descriptions,action types, and element information preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_patches = gui_dataset.to_patches(\"detections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169be266",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_patches.skip(51).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0338fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device=\"mps\")\n",
    "\n",
    "# Iterate through patches and compute embeddings\n",
    "for patch in tqdm(box_patches, desc=\"Computing embeddings\"):\n",
    "    # Construct prompt from the three fields\n",
    "    task_desc = patch.detections.task_description if patch.detections.task_description else \"\"\n",
    "    action_type = patch.detections.action_type if patch.detections.action_type else \"\"\n",
    "    element_info = patch.detections.element_info if patch.detections.element_info else \"\"\n",
    "    \n",
    "    # Combine into prompt\n",
    "    prompt = f\"clustering: Task: {task_desc}; Action: {action_type}; Element: {element_info}\"\n",
    "    \n",
    "    # Compute embedding\n",
    "    embedding = model.encode(prompt)\n",
    "    \n",
    "    # Add as new field\n",
    "    patch['text_embedding'] = embedding.tolist()\n",
    "    patch.save()\n",
    "\n",
    "print(\"Done! Embeddings added to all patches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_visualization(\n",
    "    box_patches,\n",
    "    embeddings=\"text_embedding\",\n",
    "    method=\"umap\",  # \"umap\", \"tsne\", \"pca\", etc\n",
    "    brain_key=\"text_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(box_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48495d0d",
   "metadata": {},
   "source": [
    "Let's get more hands on with the FiftyOne app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(gui_dataset)\n",
    "\n",
    "# or run this in the terminal: fiftyone app launch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
